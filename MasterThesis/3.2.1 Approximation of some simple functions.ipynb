{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ea392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5530f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(outputs_epoch, synth, title, k):\n",
    "    plt.close()\n",
    "    synth = synth.detach().numpy()\n",
    "    for i, outputs in enumerate(outputs_epoch):\n",
    "        if (i%k==1) and (i >20):\n",
    "            outputs = np.array([output.detach().numpy()[0] for output in outputs])\n",
    "            plt.plot(outputs - synth, alpha=1 - 0.5*((i+1)/len(outputs_epoch))**2, label=str(i))\n",
    "    outputs = np.array([output.detach().numpy()[0] for output in outputs_epoch[-1]])\n",
    "    plt.plot(outputs - synth, alpha=0.5, label=\"last\")\n",
    "    plt.legend()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171277d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(outputs_epoch, synth, title, k):\n",
    "    plt.close()\n",
    "    for i, outputs in enumerate(outputs_epoch):\n",
    "        if i%k==1:\n",
    "            outputs = np.array([output.detach().numpy()[0] for output in outputs])\n",
    "            plt.plot(outputs, alpha=((i+1)/len(outputs_epoch))**2, label=str(i))\n",
    "    outputs = np.array([output.detach().numpy()[0] for output in outputs_epoch[-1]])\n",
    "    plt.plot(outputs, alpha=1, label=\"last\")\n",
    "    plt.plot(synth.detach().numpy(), label=\"true\")\n",
    "    plt.legend()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ccc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_exp(x):\n",
    "    x_bar = torch.mean(x)\n",
    "    return torch.exp(2*x_bar + 3*x_bar**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28b8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_pol(x):\n",
    "    x_bar = torch.max(x)\n",
    "    return x_bar + 2*x_bar + 3*x_bar**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_cos(x):\n",
    "    x_bar = torch.mean(x)\n",
    "    return torch.cos(2*x_bar + 3*x_bar**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa9bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super().__init__() # Runs initialisation of nn.Module\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128,512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "    def phi(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = self.N\n",
    "        y = torch.zeros(N, 1)\n",
    "        for i in range(N):\n",
    "            y[i] = self.phi(x[i].view(-1))\n",
    "        y = torch.sum(y, 0) / N\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e549267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(self, N):\n",
    "        super().__init__() # Runs initialisation of nn.Module\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 128)\n",
    "        self.fc5 = nn.Linear(128, 1024)\n",
    "        \n",
    "        self.psi1 = nn.Linear(1024, 512)\n",
    "        self.psi2 = nn.Linear(512, 256)\n",
    "        self.psi3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "    def phi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def psi(self, x):\n",
    "        x = F.relu(self.psi1(x))\n",
    "        x = F.relu(self.psi2(x))\n",
    "        x = self.psi3(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = self.N\n",
    "        y = torch.zeros(N, 1024)\n",
    "        for i in range(N):\n",
    "            y[i] = self.phi(x[i].view(-1))\n",
    "        y = torch.sum(y, 0) / N\n",
    "    \n",
    "        return self.psi(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4fcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, N):\n",
    "        super().__init__() # Runs initialisation of nn.Module\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 128)\n",
    "        self.fc5 = nn.Linear(128, 1024)\n",
    "        \n",
    "        self.psi1 = nn.Linear(1024, 512)\n",
    "        self.psi2 = nn.Linear(512, 256)\n",
    "        self.psi3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "    def phi(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def psi(self, x):\n",
    "        x = F.relu(self.psi1(x))\n",
    "        x = F.relu(self.psi2(x))\n",
    "        x = self.psi3(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = self.N\n",
    "        y = torch.zeros(N, 1024)\n",
    "        for i in range(N):\n",
    "            y[i] = self.phi(x[i].view(-1))\n",
    "        y = y.max(0).values\n",
    "        return self.psi(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "n_data = 1000\n",
    "x = torch.normal(torch.zeros(n_data, N), torch.ones(n_data, N)) \n",
    "synth = torch.zeros(n_data)\n",
    "for i in range(0,n_data):\n",
    "    synth[i] = fun_cos(x[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d229d94a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoriadesmarquest/env/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(21.0915, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(23.3461, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(20.4146, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(17.4278, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(11.3501, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.4097, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.8579, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(12.2919, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(11.9594, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.5772, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(10.0172, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(6.8702, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(5.5485, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(3.1869, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(1.6258, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.8539, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(2.0254, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(6.1966, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.1875, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(2.3819, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.6669, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.3212, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.1638, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0441, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0393, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0357, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0326, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0309, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0296, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batches = 10\n",
    "eps = 3*1e-2\n",
    "net = DeepSet(N).float()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 100\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    net.zero_grad()\n",
    "    for i in range(batches):\n",
    "        k = int(n_data /batches)\n",
    "        dataX = x[i:i+k,:]\n",
    "        dataY = synth[i:i+k]\n",
    "        tot_loss = 0\n",
    "        net.zero_grad()\n",
    "        for j , (X, Y) in enumerate(zip(dataX, dataY)):\n",
    "            output = net(X)\n",
    "            loss = criterion(output, Y)\n",
    "            outputs.append(output)\n",
    "            tot_loss += loss\n",
    "        tot_loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    outputs_epoch.append(outputs)\n",
    "    print('LOSS = ', tot_loss)\n",
    "    if tot_loss < eps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bdfae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0009 < 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2edaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(outputs_epoch, synth, \"DeepSet batches\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eadd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(outputs_epoch, synth, \"DeepSet 10 batches\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecd4c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(0.0297, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0183, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0325, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0346, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0947, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.1246, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.1908, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.2005, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.2029, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = DeepSet(N)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 10\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    for X, Y in zip(x, synth):\n",
    "        net.zero_grad()\n",
    "        output = net(X)\n",
    "        outputs.append(output)\n",
    "        #print(output)\n",
    "        loss = criterion(output, Y.view(-1))\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    print('LOSS = ', loss)\n",
    "    outputs_epoch.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d11eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(outputs_epoch, synth, \"DeepSet\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24bd1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(0.4140, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.2854, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0718, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.1102, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.3298, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(1.2763, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.2824, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0687, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0205, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.4317, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = PointNet(N)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.05) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 10\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    for X, Y in zip(x, synth):\n",
    "        net.zero_grad()\n",
    "        output = net(X)\n",
    "        outputs.append(output)\n",
    "        #print(output)\n",
    "        loss = criterion(output, Y.view(-1))\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    print('LOSS = ', loss)\n",
    "    outputs_epoch.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aa5eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(outputs_epoch, synth, \"PointNet\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0e55035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(0.0583, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0414, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0373, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0388, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0442, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0494, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0513, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0499, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0609, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0605, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0608, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0581, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0631, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0614, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0693, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0616, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0617, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0681, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0772, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0798, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0730, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0832, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0821, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0744, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0471, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0507, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0774, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0490, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0827, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0480, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0481, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0479, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0467, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0464, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0464, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.1054, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0743, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0794, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0663, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0521, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0496, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0465, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0455, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0442, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0438, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0425, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0406, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0407, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0389, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0389, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0393, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0352, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0361, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0350, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0331, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0343, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0334, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0340, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0339, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0341, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0313, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0315, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0321, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0306, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0299, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0281, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0288, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0262, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0251, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0265, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0253, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0260, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0221, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0251, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0230, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0224, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0229, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0210, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0169, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0213, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0165, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0175, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "LOSS =  tensor(0.0087, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = Net(N)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 100\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    for X, Y in zip(x, synth):\n",
    "        net.zero_grad()\n",
    "        output = net(X)\n",
    "        outputs.append(output)\n",
    "        #print(output)\n",
    "        loss = criterion(output, Y.view(-1))\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    print('LOSS = ', loss)\n",
    "    outputs_epoch.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45c4c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(outputs_epoch, synth, \"Net batches\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935904f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
