{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74fcd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import NetModules\n",
    "import utils\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3357e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x, t):\n",
    "    x_bar = torch.mean(x)\n",
    "    return torch.exp(x_bar * (t + 2*t**2) + 3 * t * x_bar**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f363716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(self, N):\n",
    "        super().__init__() # Runs initialisation of nn.Module\n",
    "        self.fc1 = nn.Linear(1, 64).float()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 128)\n",
    "        self.fc5 = nn.Linear(128, 1024)\n",
    "        \n",
    "        self.psi1 = nn.Linear(1025, 512)\n",
    "        self.psi2 = nn.Linear(512, 256)\n",
    "        self.psi3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "    def phi(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def psi(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.psi1(x))\n",
    "        x = F.relu(self.psi2(x))\n",
    "        x = self.psi3(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x = x.float()\n",
    "        N = self.N\n",
    "        y = torch.zeros(N, 1024)\n",
    "        for i in range(N):\n",
    "            y[i] = self.phi(x[i].view(-1).float())\n",
    "        y = torch.sum(y, 0) / N\n",
    "        y = torch.cat([y, t])\n",
    "        return self.psi(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41527e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "n_data = 1000\n",
    "x = torch.normal(torch.zeros(n_data, N), torch.ones(n_data, N)).float()\n",
    "t = torch.tensor(np.linspace(0,1, n_data)).view(-1, 1)\n",
    "synth = torch.zeros(n_data)\n",
    "for i in range(0,n_data):\n",
    "    synth[i] = fun(x[i, :], t[i])\n",
    "synth = synth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9866c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0136, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0032, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0055, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(3.2362e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(3.4088e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.0990e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0085, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0046, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0125, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(7.1043e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0007, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(2.0593e-06, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(2.6797e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(7.9162e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(7.2666e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(5.3941e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.6156e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.3802e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.3659e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.2663e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.5035e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.7716e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.4677e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.3215e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.7584e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.4336e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.4395e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.4649e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(5.1485e-06, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(3.1940e-06, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(1.8994e-06, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.5985e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(6.2158e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(3.2902e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.7298e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0083, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0038, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0035, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0124, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0086, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.8731e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.4629e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(6.4329e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.7741e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(9.0094e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.8255e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(7.7861e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(5.7012e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.4186e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(5.5717e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.5154e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.8730e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(1.6850e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(8.9169e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(2.8694e-05, grad_fn=<AddBackward0>)\n",
      "LOSS =  tensor(4.8973e-05, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batches = 10\n",
    "net = DeepSet(N).float()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 100\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    net.zero_grad()\n",
    "    for i in range(batches):\n",
    "        k = int(n_data /batches)\n",
    "        dataX = x[i:i+k,:]\n",
    "        dataY = synth[i:i+k]\n",
    "        tot_loss = 0\n",
    "        net.zero_grad()\n",
    "        l = 0\n",
    "        for j , (X, Y) in enumerate(zip(dataX, dataY)):\n",
    "            output = net(X, t[l])\n",
    "            loss = criterion(output, Y)\n",
    "            l += 1\n",
    "            outputs.append(output)\n",
    "        tot_loss += loss\n",
    "        tot_loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    outputs_epoch.append(outputs)\n",
    "    print('LOSS = ', tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d7fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(outputs_epoch, synth, title, k):\n",
    "    plt.close()\n",
    "    for i, outputs in enumerate(outputs_epoch):\n",
    "        if i%k==1:\n",
    "            outputs = np.array([output.detach().numpy()[0] for output in outputs])\n",
    "            plt.plot(outputs, alpha=((i+1)/len(outputs_epoch))**2, label=str(i))\n",
    "    outputs = np.array([output.detach().numpy()[0] for output in outputs_epoch[-1]])\n",
    "    plt.plot(outputs, alpha=1, label=\"last\")\n",
    "    plt.plot(synth.detach().numpy(), label=\"true\")\n",
    "    plt.legend()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c1b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(outputs_epoch, synth, \"DeepSet 10 batches\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9602fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs_epoch))\n",
    "print(len(outputs_epoch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8f565c86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS =  tensor(401.8682, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-91ae18505e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#print(synth, output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtot_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtot_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Backpropagate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adjusts the steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moutputs_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = DeepSet(N).float()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # Corresponds to evything that is adjustable\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 10\n",
    "outputs_epoch = []\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = []\n",
    "    for dataX, dataY in zip(trainloaderX, trainloaderY):\n",
    "        #for j, d in enumerate(data):\n",
    "        tot_loss = 0\n",
    "        for j, (X, Y) in enumerate(zip(dataX, dataY)):\n",
    "            net.zero_grad()\n",
    "            #print(synth)\n",
    "            #print(t[j].shape)\n",
    "            #print(X.shape)\n",
    "            output = net(X, t[j])\n",
    "            outputs.append(output)\n",
    "            #print(output)\n",
    "            loss = criterion(output, Y)\n",
    "            #print(synth, output)\n",
    "            tot_loss += loss\n",
    "        tot_loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Adjusts the steps\n",
    "    outputs_epoch.append(outputs)\n",
    "    print('LOSS = ', tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8de28488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloaderX))\n",
    "print(len(dataX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "18426470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(outputs_epoch[0]))\n",
    "len(synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c284a3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd424220b20>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, outputs in enumerate(outputs_epoch):\n",
    "    if i%5==1:\n",
    "        outputs = np.array([output.detach().numpy()[0] for output in outputs])\n",
    "        plt.plot(outputs, label=str(i))\n",
    "plt.plot(synth.detach().numpy(), label=\"true\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570aa21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
